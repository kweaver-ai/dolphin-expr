#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PromptOptimizer usage examples.

Demonstrates how to use PromptOptimizer to optimize an agent's .dph file.
"""
import sys
from pathlib import Path

# Add repo root to sys.path
sys.path.insert(0, str(Path(__file__).resolve().parents[2]))

from optimization import (
    PromptOptimizer,
    QuickPromptOptimizer,
    DeepPromptOptimizer,
    Budget
)


class MockLLMClient:
    """Mock LLM client (replace with a real one in production)."""

    def generate(self, prompt: str) -> str:
        """Generate an optimized prompt."""
        # In real usage, call an LLM API here
        return "ä¼˜åŒ–åçš„ prompt å†…å®¹..."


class MockSemanticJudge:
    """Mock SemanticJudge (replace with a real one in production)."""

    def evaluate(self, analysis_content: str, expected: str, actual: str, knowledge: str = '') -> dict:
        """Return an evaluation result compatible with the SemanticJudge interface."""
        # Simple mock implementation
        if expected and expected.lower() in actual.lower():
            score = 1.0
        else:
            score = 0.5

        from optimization.types import SemanticJudgeDetail

        return {
            'score': score,
            'details': SemanticJudgeDetail(
                error_types=[],
                action_vector=['improve clarity'],
                candidate_injects=[],
                rationale='Mock evaluation',
                phase='exact'
            )
        }


def example_1_basic_usage():
    """Example 1: basic usage."""
    print("=" * 70)
    print("  ç¤ºä¾‹ 1: PromptOptimizer åŸºæœ¬ä½¿ç”¨")
    print("=" * 70)
    print()

    # 1. Create LLM client and SemanticJudge
    llm_client = MockLLMClient()
    semantic_judge = MockSemanticJudge()

    # 2. Create optimizer
    optimizer = PromptOptimizer.create_default(
        llm_client=llm_client,
        semantic_judge=semantic_judge,
        target_section='system'  # Optimize system section only
    )

    # 3. Prepare context
    context = {
        'agent_path': 'path/to/agent.dph',
        'failed_cases': [
            {'case_id': '001', 'error_type': 'logic_error'},
            {'case_id': '002', 'error_type': 'tool_misuse'}
        ],
        'knowledge': 'ä¸šåŠ¡è§„åˆ™ï¼š...',
        'error_types': ['logic_error', 'tool_misuse']
    }

    # 4. Set budget
    budget = Budget(max_iters=3, max_seconds=180)

    # 5. Optimize agent content
    target = """
system = \"\"\"
ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æåŠ©æ‰‹ã€‚
\"\"\"
"""

    print("ä¼˜åŒ–åŸå§‹å†…å®¹...")
    result = optimizer.optimize(target, context, budget)

    # 6. Inspect results
    print(f"\nâœ“ ä¼˜åŒ–å®Œæˆï¼")
    print(f"  æœ€ä½³å¾—åˆ†: {result.best_score:.2f}")
    if result.best_candidate:
        print(f"  ä¼˜åŒ–åçš„å†…å®¹:\n{result.best_candidate.content[:200]}...")
    print()


def example_2_optimize_file():
    """Example 2: optimize a file (with backup)."""
    print("=" * 70)
    print("  ç¤ºä¾‹ 2: ä¼˜åŒ– Agent æ–‡ä»¶ï¼ˆå¸¦å¤‡ä»½ï¼‰")
    print("=" * 70)
    print()

    llm_client = MockLLMClient()
    semantic_judge = MockSemanticJudge()

    # Create optimizer
    optimizer = PromptOptimizer.create_default(
        llm_client=llm_client,
        semantic_judge=semantic_judge
    )

    # NOTE: this uses a demo path; in real usage, provide a real file path
    print("âš ï¸  è¿™æ˜¯ä¸€ä¸ªæ¼”ç¤ºç¤ºä¾‹ï¼Œä½¿ç”¨çš„æ˜¯æ¨¡æ‹Ÿè·¯å¾„")
    print()

    # Example snippet (uncomment for real usage)
    print("ä½¿ç”¨ç¤ºä¾‹ä»£ç ï¼š")
    print("""
    result = optimizer.optimize_file(
        agent_path='design/watsons_baseline/dolphins/my_agent.dph',
        context={
            'failed_cases': failed_cases,
            'knowledge': business_rules,
            'error_types': ['logic_error']
        },
        budget=Budget(max_iters=5, max_seconds=300),
        backup=True,      # Auto-backup original file to .backup/ directory
        replace=False     # Do not auto-replace (inspect result first)
    )

    if result.best_candidate:
        print(f"âœ“ ä¼˜åŒ–æˆåŠŸï¼æœ€ä½³å¾—åˆ†: {result.best_score:.2f}")

        # Inspect optimized content
        print("ä¼˜åŒ–åçš„å†…å®¹:")
        print(result.best_candidate.content)

        # If satisfied, replace manually
        # agent_path.write_text(result.best_candidate.content)
    """)
    print()


def example_3_quick_vs_deep():
    """Example 3: quick optimization vs deep optimization."""
    print("=" * 70)
    print("  ç¤ºä¾‹ 3: å¿«é€Ÿä¼˜åŒ– vs æ·±åº¦ä¼˜åŒ–")
    print("=" * 70)
    print()

    llm_client = MockLLMClient()
    semantic_judge = MockSemanticJudge()

    # Quick optimizer: fewer candidates, faster convergence
    quick_optimizer = QuickPromptOptimizer(
        llm_client=llm_client,
        semantic_judge=semantic_judge
    )

    # Deep optimizer: more candidates, best quality
    deep_optimizer = DeepPromptOptimizer(
        llm_client=llm_client,
        semantic_judge=semantic_judge
    )

    target = """
system = \"\"\"
ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ã€‚
\"\"\"
"""

    context = {
        'agent_path': 'test.dph',
        'error_types': ['logic_error']
    }

    print("1. å¿«é€Ÿä¼˜åŒ–ï¼ˆ3 ä¸ªåˆå§‹å€™é€‰ï¼Œè€å¿ƒå€¼=1ï¼‰")
    quick_budget = Budget(max_iters=3)
    quick_result = quick_optimizer.optimize(target, context, quick_budget)
    print(f"   å¾—åˆ†: {quick_result.best_score:.2f}")
    print()

    print("2. æ·±åº¦ä¼˜åŒ–ï¼ˆ10 ä¸ªåˆå§‹å€™é€‰ï¼Œè€å¿ƒå€¼=5ï¼‰")
    deep_budget = Budget(max_iters=10)
    deep_result = deep_optimizer.optimize(target, context, deep_budget)
    print(f"   å¾—åˆ†: {deep_result.best_score:.2f}")
    print()

    print("å¯¹æ¯”ï¼š")
    print(f"  å¿«é€Ÿä¼˜åŒ– - è¿­ä»£: {len(quick_result.optimization_history)}, å¾—åˆ†: {quick_result.best_score:.2f}")
    print(f"  æ·±åº¦ä¼˜åŒ– - è¿­ä»£: {len(deep_result.optimization_history)}, å¾—åˆ†: {deep_result.best_score:.2f}")
    print()


def example_4_custom_configuration():
    """Example 4: custom configuration."""
    print("=" * 70)
    print("  ç¤ºä¾‹ 4: è‡ªå®šä¹‰ä¼˜åŒ–å™¨é…ç½®")
    print("=" * 70)
    print()

    llm_client = MockLLMClient()
    semantic_judge = MockSemanticJudge()

    # Custom configuration
    optimizer = PromptOptimizer(
        llm_client=llm_client,
        semantic_judge=semantic_judge,
        target_section='system',     # Optimize system section only
        initial_size=5,              # 5 initial candidates
        use_two_phase=True,          # Use two-phase evaluation (cost optimization)
        patience=3,                  # Patience = 3
        min_improvement=0.05         # Minimum improvement = 5%
    )

    print("è‡ªå®šä¹‰é…ç½®ï¼š")
    print(f"  - ç›®æ ‡éƒ¨åˆ†: system")
    print(f"  - åˆå§‹å€™é€‰æ•°: 5")
    print(f"  - ä¸¤é˜¶æ®µè¯„ä¼°: æ˜¯ï¼ˆå…ˆå¿«é€Ÿç­›é€‰ï¼Œå†ç²¾ç¡®è¯„ä¼°ï¼‰")
    print(f"  - æ—©åœè€å¿ƒå€¼: 3")
    print(f"  - æœ€å°æ”¹è¿›: 5%")
    print()

    # Example: optimize system prompt
    target = """
system = \"\"\"
ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æåŠ©æ‰‹ã€‚
è¯·å¸®åŠ©ç”¨æˆ·åˆ†ææ•°æ®å¹¶å›ç­”é—®é¢˜ã€‚
\"\"\"
"""

    context = {
        'agent_path': 'agent.dph',
        'failed_cases': [
            {'question': 'Q1', 'expected': 'A1', 'actual': 'Wrong'},
            {'question': 'Q2', 'expected': 'A2', 'actual': 'Wrong'}
        ],
        'knowledge': 'åˆ†æè§„åˆ™ï¼š...',
        'error_types': ['logic_error', 'missing_info']
    }

    budget = Budget(max_iters=5, max_seconds=300)

    print("è¿è¡Œä¼˜åŒ–...")
    result = optimizer.optimize(target, context, budget)

    print(f"âœ“ å®Œæˆï¼æœ€ä½³å¾—åˆ†: {result.best_score:.2f}")
    print()


def main():
    """Main entry point."""
    print("\n")
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘            PromptOptimizer ä½¿ç”¨ç¤ºä¾‹                              â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()

    # Run examples
    example_1_basic_usage()
    example_2_optimize_file()
    example_3_quick_vs_deep()
    example_4_custom_configuration()

    print("=" * 70)
    print("  æ›´å¤šä¿¡æ¯")
    print("=" * 70)
    print()
    print("ğŸ“– å®Œæ•´æ–‡æ¡£:")
    print("  - docs/optimization.md")
    print("  - baks/optimization/OPTIMIZATION_METHODS.md")
    print("  - baks/optimization/PHASE2_IMPLEMENTATION_SUMMARY.md")
    print()
    print("ğŸ’¡ æç¤º:")
    print("  - å¿«é€Ÿä¼˜åŒ–ï¼šä½¿ç”¨ QuickPromptOptimizer")
    print("  - æ·±åº¦ä¼˜åŒ–ï¼šä½¿ç”¨ DeepPromptOptimizer")
    print("  - æˆæœ¬ä¼˜åŒ–ï¼šå¯ç”¨ä¸¤é˜¶æ®µè¯„ä¼°ï¼ˆuse_two_phase=Trueï¼‰")
    print("  - å®‰å…¨ç¬¬ä¸€ï¼šå§‹ç»ˆå…ˆ backup=True, replace=False")
    print()


if __name__ == '__main__':
    main()
