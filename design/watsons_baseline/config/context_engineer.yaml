model:
  context_limit: 64000          # Model context window (input + output)
  output_target: 4096           # Tokens reserved for output

# ============================================
buckets:
  # Stable system prompt
  _system:
    weight: 2.0
    message_role: system
    compress: none               # Do not compress system prompt; keep stable for prefix cache

  # Current user input
  _query:
    weight: 1.0
    message_role: user
    min_tokens: 1000
    max_tokens: 20000  

  # Unified conversation history (all explore/llm history aggregated here)
  conversation_history:
    weight: 2.5
    message_role: user
    
  # Temporary reasoning/tool outputs (kept short)
  _scratchpad:
    weight: 1.0
    message_role: user

policies:
  default:
    # Bucket order: system -> conversation_history -> query -> scratchpad
    bucket_order: ["_system", "conversation_history", "_query", "_scratchpad"]
    # Drop priority: scratchpad first, then older conversation history
    drop_order: ["_scratchpad", "conversation_history"]
