#!/usr/bin/env python3
# -*- coding: utf-8 -*-
from __future__ import annotations

"""
Inject content optimizer (semantic-driven only).

Core idea:
Generate inject content and control convergence based on SemanticJudge diagnostics
(score, error_types, action_vector, candidate_injects). The loss is defined as
semantic loss: loss = 1 - score.

Safety: do not leak answers into inject content.
"""

import re
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from enum import Enum
import random

try:
    from .semantic_gradient import SemanticGradient
except ImportError:
    from semantic_gradient import SemanticGradient


class ErrorType(Enum):
    """Error type categories."""

    FIELD_ERROR = "field_error"  # Field usage error
    CALCULATION_ERROR = "calc_error"  # Calculation/logic error
    FORMAT_ERROR = "format_error"  # Output format error
    INCOMPLETE = "incomplete"  # Incomplete result
    MAGNITUDE_ERROR = "magnitude"  # Order-of-magnitude error
    LOGIC_ERROR = "logic_error"  # Reasoning/logic error
    TIMEOUT_ERROR = "timeout_error"  # Execution timeout
    UNKNOWN = "unknown"


@dataclass
class FailureRecord:
    """Failure record."""

    iteration: int
    inject_content: str
    actual_output: str
    error_type: ErrorType
    error_features: Dict
    loss: float


@dataclass
class OptimizationInfo:
    """Optimization info."""

    gradient: Dict
    learning_rate: float
    convergence_status: str
    loss: float
    momentum_strength: float


class InjectsOptimizer:
    """
    Inject content optimizer inspired by gradient descent.
    """

    def __init__(
        self,
        learning_rate: float = 1.0,
        momentum: float = 0.9,
        patience: int = 3,
        min_learning_rate: float = 0.1,
    ):
        self.learning_rate = learning_rate
        self.initial_learning_rate = learning_rate
        self.momentum = momentum
        self.patience = patience
        self.min_learning_rate = min_learning_rate

        # State tracking
        self.velocity = {}
        self.failure_history: List[FailureRecord] = []
        self.loss_history: List[float] = []
        self.best_loss = float("inf")
        self.plateau_count = 0

        # Baseline tracking
        self.baseline_result = None
        self.baseline_loss = None

        # Semantic-driven context (optional)
        self._semantic_judge: Any | None = None
        self._semantic_analysis_content: str = ""
        self._semantic_knowledge: str = ""

    def optimize(
        self,
        actual: str,
        expected: str,
        knowledge_base: str,
        iteration: int,
        previous_inject: str = "",
    ) -> Tuple[str, OptimizationInfo]:
        """
        Main optimization entry.

        Returns:
            (optimized inject content, optimization info)
        """
        # Semantic-driven mode (must be enabled)
        if not (self._semantic_judge is not None):
            raise RuntimeError(
                "Semantic mode not enabled. Call enable_semantic(...) before optimize()."
            )

        judge_grad_raw = self._semantic_judge.evaluate(
            analysis_content=self._semantic_analysis_content or "",
            expected=expected or "",
            actual=actual or "",
            knowledge=self._semantic_knowledge or knowledge_base or "",
        )

        semantic_grad = SemanticGradient.from_judge_result(judge_grad_raw)
        gradient = self._build_semantic_gradient(semantic_grad, actual, expected)
        loss = semantic_grad.loss
        self.loss_history.append(loss)
        self._update_momentum(gradient)
        adaptive_lr = self._get_adaptive_learning_rate(iteration)
        new_inject = self._generate_inject_from_gradient(
            gradient, knowledge_base, iteration, adaptive_lr
        )
        self._audit_inject(new_inject, expected)
        self._record_failure(iteration, new_inject, actual, gradient, loss)
        opt_info = OptimizationInfo(
            gradient=gradient,
            learning_rate=adaptive_lr,
            convergence_status=self._check_convergence(),
            loss=loss,
            momentum_strength=self._get_momentum_strength(),
        )
        return new_inject, opt_info

    def enable_semantic(self, judge: Any, analysis_content: str, knowledge: str = ""):
        """Enable semantic-driven mode.

        Args:
            judge: SemanticJudge instance (must provide evaluate()).
            analysis_content: Cross-run summary analysis.
            knowledge: Domain knowledge text.
        """
        self._semantic_judge = judge
        self._semantic_analysis_content = analysis_content or ""
        self._semantic_knowledge = knowledge or ""

    def _build_semantic_gradient(
        self, semantic_grad: SemanticGradient, actual: str, expected: str
    ) -> Dict:
        """Map SemanticGradient to an internal gradient structure."""
        score = semantic_grad.score
        error_types = semantic_grad.error_types
        action_vector = semantic_grad.action_vector
        candidate_injects = semantic_grad.candidate_injects

        et = self._map_semantic_error_to_enum(
            error_types[0] if error_types else "unknown"
        )
        gradient = {
            "has_output": bool(actual and actual.strip()),
            "is_error": self._is_error_output(actual),
            "looks_complete": not (actual or "").endswith(("...", "çœç•¥")),
            "has_content": len(actual or "") > 20,
            "semantic_hint": (
                ("needs_refinement" if score < 0.6 else "ok") if actual else "no_output"
            ),
            "error_type": et,
            "magnitude": 1.0 - score,
            "action_vector": action_vector,
            "candidate_injects": candidate_injects,
            "score": score,
            "raw": semantic_grad.to_dict(),
        }

        if self.baseline_result is not None and self.baseline_loss is not None:
            current_loss = 1.0 - score
            baseline_loss = self.baseline_loss
            gradient["improved_from_baseline"] = current_loss < baseline_loss
            gradient["improvement_ratio"] = (
                (baseline_loss - current_loss) / baseline_loss
                if baseline_loss > 0
                else 0
            )
            gradient["baseline_comparison"] = {
                "improvement": gradient["improvement_ratio"],
                "baseline_loss": baseline_loss,
                "current_loss": current_loss,
                "is_better": current_loss < baseline_loss,
                "degradation_ratio": (
                    (current_loss - baseline_loss) / baseline_loss
                    if baseline_loss > 0
                    else 0
                ),
            }

        return gradient

    def _map_semantic_error_to_enum(self, err: str) -> ErrorType:
        """Map SemanticJudge error-type strings to internal enum values."""
        if not err:
            return ErrorType.UNKNOWN
        e = err.lower()
        if any(k in e for k in ["calc", "è®¡ç®—", "æ•°å€¼", "å…¬å¼"]):
            return ErrorType.CALCULATION_ERROR
        if any(k in e for k in ["å­—æ®µ", "field", "åˆ—", "ç»´åº¦"]):
            return ErrorType.FIELD_ERROR
        if any(k in e for k in ["æ ¼å¼", "format", "è¾“å‡ºæ ¼å¼"]):
            return ErrorType.FORMAT_ERROR
        if any(k in e for k in ["ä¸å®Œæ•´", "ç¼ºå¤±", "incomplete"]):
            return ErrorType.INCOMPLETE
        if any(k in e for k in ["æ•°é‡çº§", "magnitude"]):
            return ErrorType.MAGNITUDE_ERROR
        if any(k in e for k in ["é€»è¾‘", "reasoning", "logic"]):
            return ErrorType.LOGIC_ERROR
        if any(k in e for k in ["è¶…æ—¶", "timeout"]):
            return ErrorType.TIMEOUT_ERROR
        return ErrorType.UNKNOWN

    def set_baseline(self, baseline_result: str, baseline_loss: float):
        """Set baseline values for comparison."""
        self.baseline_result = baseline_result
        self.baseline_loss = baseline_loss
        print(f"ğŸ“Š è®¾ç½®baseline: æŸå¤±={baseline_loss:.4f}")

    # Removed: surface-statistics loss function; use semantic loss (1 - score)
    # Removed: heuristic gradient calculation; unified to SemanticJudge-driven gradient

    def _is_error_output(self, output: str) -> bool:
        """Heuristically determine whether this is an error output."""
        if not output:
            return True
        lower_output = output.lower()
        return any(
            err in lower_output
            for err in ["error", "exception", "failed", "é”™è¯¯", "å¤±è´¥"]
        )

    def _count_stuck_iterations(self) -> int:
        """Count consecutive iterations stuck in the same error type."""
        if len(self.failure_history) < 2:
            return 0

        # Count consecutive identical errors
        current_error = self.failure_history[-1].error_type
        stuck_count = 0
        for record in reversed(self.failure_history):
            if record.error_type == current_error:
                stuck_count += 1
            else:
                break
        return stuck_count

    # Removed: _classify_error/_classify_error_simple and basic feature methods (not needed in semantic mode)
    # Removed: heuristic optimization directions (replaced by action_vector/candidate_injects)

    def _update_momentum(self, gradient: Dict):
        """Update momentum."""
        error_type_key = gradient["error_type"].value

        if "error_type" not in self.velocity:
            self.velocity["error_type"] = {}

        if error_type_key not in self.velocity["error_type"]:
            self.velocity["error_type"][error_type_key] = 0

        # Update momentum
        self.velocity["error_type"][error_type_key] = (
            self.momentum * self.velocity["error_type"][error_type_key]
            + (1 - self.momentum) * gradient["magnitude"]
        )

    def _generate_inject_from_gradient(
        self, gradient: Dict, knowledge_base: str, iteration: int, learning_rate: float
    ) -> str:
        """
        Inject generation: prefer semantic candidates/action vectors, otherwise fall back to hint composition.
        """
        # Prefer candidate injects
        cand = gradient.get("candidate_injects") or []
        if isinstance(cand, list) and cand:
            return cand[0]

        # Otherwise use action vectors
        actions = gradient.get("action_vector") or []
        if isinstance(actions, list) and actions:
            return "ï¼›".join(actions)

        inject_parts = []

        # Simple guidance based on semantic hints
        semantic_hint = gradient.get("semantic_hint", "needs_refinement")

        if semantic_hint == "no_output":
            inject_parts.append("è¯·ç¡®ä¿æä¾›æœ‰æ•ˆçš„è¾“å‡ºç»“æœ")
        elif semantic_hint == "execution_error":
            inject_parts.append("è¯·æ£€æŸ¥å¹¶ä¿®æ­£æ‰§è¡Œè¿‡ç¨‹ä¸­çš„é”™è¯¯")
        elif semantic_hint == "incomplete_response":
            inject_parts.append("è¯·ç¡®ä¿è¾“å‡ºå®Œæ•´çš„åˆ†æç»“æœ")
        elif semantic_hint == "too_brief":
            inject_parts.append("è¯·æä¾›æ›´è¯¦ç»†çš„åˆ†æå’Œè¯´æ˜")
        else:
            inject_parts.append("è¯·ä»”ç»†æ£€æŸ¥åˆ†æè´¨é‡ï¼Œç¡®ä¿å‡†ç¡®æ€§")

        # Adjust strategy based on failure count
        failure_count = gradient.get("failure_count", 0)
        if failure_count > 2:
            inject_parts.append("è¯·å°è¯•ä¸åŒçš„åˆ†ææ–¹æ³•")

        stuck_count = gradient.get("stuck_iterations", 0)
        if stuck_count > 1:
            inject_parts.append("è¯·ä»æ–°çš„è§’åº¦é‡æ–°æ€è€ƒé—®é¢˜")

        # Add knowledge base content (simplified)
        if knowledge_base and len(knowledge_base) > 50:
            # Use only the first 200 characters and let the LLM infer relevance
            knowledge_excerpt = knowledge_base[:200].strip()
            if knowledge_excerpt:
                inject_parts.append(f"å‚è€ƒè¦ç‚¹ï¼š{knowledge_excerpt}")

        # Guidance based on baseline comparison
        if gradient.get("improved_from_baseline", False):
            improvement = gradient.get("improvement_ratio", 0)
            if improvement > 0.3:
                inject_parts.append("å½“å‰æ–¹å‘æ­£ç¡®ï¼Œç»§ç»­ä¼˜åŒ–")
            else:
                inject_parts.append("æœ‰æ‰€æ”¹è¿›ä½†ä»éœ€è¿›ä¸€æ­¥ä¼˜åŒ–")
        elif gradient.get("improvement_ratio", 0) < 0:
            inject_parts.append("å½“å‰æ–¹æ³•å¯èƒ½æœ‰é—®é¢˜ï¼Œè¯·è°ƒæ•´ç­–ç•¥")

        # Combine all parts
        base_inject = (
            "ï¼›".join(inject_parts) if inject_parts else "è¯·ä»”ç»†åˆ†æå¹¶æä¾›å‡†ç¡®ç»“æœ"
        )

        # Increase emphasis based on iteration count
        if iteration > 0:
            base_inject = f"ç¬¬{iteration + 1}æ¬¡æé†’ï¼š{base_inject}"

        return base_inject

    # Removed: heuristic helpers such as knowledge extraction/history learning

    def _generate_baseline_guidance(self, baseline_comparison: Dict) -> str:
        """Generate guidance based on baseline comparison."""
        if not baseline_comparison:
            return ""

        guidance_parts = []

        # Provide generic guidance based on improvement/degradation magnitude
        if not baseline_comparison.get("is_better", False):
            degradation_ratio = baseline_comparison.get("degradation_ratio", 0)
            if degradation_ratio > 0.2:
                guidance_parts.append("å½“å‰æ–¹æ³•å¯èƒ½å¯¼è‡´ç»“æœé€€åŒ–ï¼Œè¯·è°ƒæ•´ç­–ç•¥")
            else:
                guidance_parts.append("æ”¹è¿›æœ‰é™ï¼Œå»ºè®®å°è¯•ä¸åŒçš„æ¨ç†è·¯å¾„")
        else:
            guidance_parts.append("æ–¹å‘æ­£ç¡®ï¼Œå»ºè®®ç»§ç»­æ²¿æ­¤æ–¹å‘ç»†åŒ–")

        # If improvement is small, suggest more aggressive strategy changes
        improvement = baseline_comparison.get("improvement", 0)
        if 0 < improvement < 0.1:  # Small improvement
            guidance_parts.append("éœ€è¦æ›´å¤§çš„ç­–ç•¥è°ƒæ•´æ¥å®ç°çªç ´")

        if guidance_parts:
            return f"åŸºäºbaselineå¯¹æ¯”ï¼š{' '.join(guidance_parts)}"
        return ""

    # Removed: failure-mode and repeated-error heuristics (not needed in semantic mode)

    def _get_adaptive_learning_rate(self, iteration: int) -> float:
        """Adaptive learning rate."""
        # Base decay
        decay_rate = 0.9
        base_lr = self.initial_learning_rate * (decay_rate**iteration)

        # Adjust based on convergence status
        if self._check_convergence() == "stuck":
            base_lr *= 1.5  # Increase exploration
        elif self._check_convergence() == "oscillating":
            base_lr *= 0.5  # Reduce oscillation

        return max(base_lr, self.min_learning_rate)

    def _check_convergence(self) -> str:
        """Check convergence status."""
        if len(self.loss_history) < 3:
            return "initializing"

        recent_losses = self.loss_history[-3:]

        # Check whether we are stuck
        if len(self.failure_history) >= 2:
            recent_errors = [f.error_type for f in self.failure_history[-2:]]
            if recent_errors[0] == recent_errors[1]:
                return "stuck"

        # Check whether we are oscillating
        if len(recent_losses) >= 3:
            if (
                recent_losses[0] < recent_losses[1] > recent_losses[2]
                or recent_losses[0] > recent_losses[1] < recent_losses[2]
            ):
                return "oscillating"

        # Check whether we are improving
        if recent_losses[-1] < recent_losses[0]:
            return "improving"

        return "plateau"

    def should_early_stop(self) -> bool:
        """Check whether early stopping should trigger."""
        if len(self.loss_history) < self.patience:
            return False

        # Check whether there has been no improvement recently
        recent_losses = self.loss_history[-self.patience :]
        if max(recent_losses) - min(recent_losses) < 0.01:
            self.plateau_count += 1
            return self.plateau_count >= 2

        self.plateau_count = 0
        return False

    def random_exploration(self, knowledge_base: str) -> str:
        """Randomly explore a new strategy."""
        strategies = [
            "è¯·ä»ä¸åŒè§’åº¦é‡æ–°åˆ†æé—®é¢˜",
            "å»ºè®®ç®€åŒ–æŸ¥è¯¢é€»è¾‘ï¼Œåˆ†æ­¥éª¤æ‰§è¡Œ",
            "è¯·æ£€æŸ¥æ˜¯å¦é—æ¼äº†å…³é”®çš„è¿‡æ»¤æ¡ä»¶",
            "å°è¯•ä½¿ç”¨ä¸åŒçš„æ•°æ®èšåˆæ–¹å¼",
        ]

        base_strategy = random.choice(strategies)

        if knowledge_base and len(knowledge_base) > 100:
            # Randomly sample a portion of the knowledge base
            start_pos = random.randint(0, len(knowledge_base) - 100)
            knowledge_fragment = knowledge_base[start_pos : start_pos + 200]
            return f"{base_strategy}ã€‚å‚è€ƒçŸ¥è¯†ï¼š{knowledge_fragment}"

        return base_strategy

    def _record_failure(
        self,
        iteration: int,
        inject_content: str,
        actual_output: str,
        gradient: Dict,
        loss: float,
    ):
        """Record failure information."""
        record = FailureRecord(
            iteration=iteration,
            inject_content=inject_content,
            actual_output=actual_output[:500],  # Length cap
            error_type=gradient["error_type"],
            error_features=gradient.get("features", {}),
            loss=loss,
        )
        self.failure_history.append(record)

        # Keep only the most recent records
        if len(self.failure_history) > 10:
            self.failure_history = self.failure_history[-10:]

    def _audit_inject(self, inject_content: str, expected: str):
        """Audit inject content to ensure it does not contain the answer."""
        # Check direct answer keywords
        dangerous_patterns = [
            r"ç­”æ¡ˆæ˜¯",
            r"ç»“æœæ˜¯.*\d",  # Only match cases where digits follow "ç»“æœæ˜¯"
            r"åº”è¯¥æ˜¯",
            r"æ­£ç¡®ç­”æ¡ˆ",
            r"è®¡ç®—ç»“æœ.*\d",  # Only match cases where digits follow "è®¡ç®—ç»“æœ"
        ]

        for pattern in dangerous_patterns:
            if re.search(pattern, inject_content):
                raise ValueError(f"æ³¨å…¥å†…å®¹åŒ…å«å±é™©æ¨¡å¼: {pattern}")

        # Extract key fragments from expected (avoid false positives for short terms)
        answer_fragments = []
        words = expected.split()
        for i in range(len(words)):
            for j in range(i + 2, min(i + 6, len(words) + 1)):  # 2-5 word fragments
                fragment = " ".join(words[i:j])
                if len(fragment) > 8:  # Only check longer fragments to reduce false positives
                    answer_fragments.append(fragment)

        for fragment in answer_fragments:
            if fragment.lower() in inject_content.lower():
                raise ValueError(f"æ³¨å…¥å†…å®¹å¯èƒ½æ³„éœ²ç­”æ¡ˆç‰‡æ®µ: {fragment[:20]}...")

        # Check concrete numbers (>=3 digits)
        expected_numbers = re.findall(r"\d{3,}", expected)
        inject_numbers = re.findall(r"\d{3,}", inject_content)

        for num in inject_numbers:
            if num in expected_numbers:
                raise ValueError(f"æ³¨å…¥å†…å®¹åŒ…å«ç­”æ¡ˆä¸­çš„å…·ä½“æ•°å€¼: {num}")

        # Check percentages
        expected_percentages = re.findall(r"\d+%", expected)
        inject_percentages = re.findall(r"\d+%", inject_content)

        for pct in inject_percentages:
            if pct in expected_percentages:
                raise ValueError(f"æ³¨å…¥å†…å®¹åŒ…å«ç­”æ¡ˆä¸­çš„å…·ä½“ç™¾åˆ†æ¯”: {pct}")

        return True

    # Removed: format-type detection (heuristics)
    # Removed: format/structure-related heuristics
    # Removed: structure/completeness/numeric-presence/magnitude heuristics

    def _analyze_previous_inject(self, previous_inject: str) -> Dict:
        """Analyze previous inject content."""
        if not previous_inject:
            return {}

        return {
            "length": len(previous_inject),
            "has_emphasis": any(
                word in previous_inject for word in ["é‡è¦", "æ³¨æ„", "å…³é”®"]
            ),
            "has_specific_guidance": len(
                re.findall(r"å­—æ®µ|è¡¨|è®¡ç®—|å…¬å¼", previous_inject)
            )
            > 0,
            "iteration_mentioned": bool(re.search(r"ç¬¬\d+æ¬¡", previous_inject)),
        }

    def _combine_inject_components(self, components: List[str]) -> str:
        """Combine inject components."""
        if not components:
            return "è¯·ä»”ç»†åˆ†æé—®é¢˜å¹¶ç»™å‡ºå‡†ç¡®ç»“æœ"

        # De-duplicate while preserving order
        unique_components = list(dict.fromkeys(components))  # Ordered de-dup
        return "ï¼›".join(unique_components)

    # Removed: heuristic baseline comparison (semantic mode handles this inside _build_semantic_gradient)

    def _get_momentum_strength(self) -> float:
        """Get current momentum strength."""
        if not self.velocity.get("error_type"):
            return 0.0

        return sum(self.velocity["error_type"].values()) / len(
            self.velocity["error_type"]
        )

    def get_optimization_summary(self) -> Dict:
        """Get an optimization summary."""
        if not self.failure_history:
            return {}

        error_types = [f.error_type.value for f in self.failure_history]
        error_counts = {et: error_types.count(et) for et in set(error_types)}

        return {
            "total_iterations": len(self.failure_history),
            "error_distribution": error_counts,
            "loss_trend": (
                self.loss_history[-5:]
                if len(self.loss_history) >= 5
                else self.loss_history
            ),
            "best_loss": self.best_loss,
            "final_convergence_status": self._check_convergence(),
            "momentum_info": self.velocity,
        }
