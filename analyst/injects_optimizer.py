#!/usr/bin/env python3
# -*- coding: utf-8 -*-
from __future__ import annotations

"""
æ³¨å…¥å†…å®¹ä¼˜åŒ–å™¨ï¼ˆçº¯è¯­ä¹‰é©±åŠ¨ï¼‰

æ ¸å¿ƒæ€æƒ³ï¼šåŸºäº SemanticJudge çš„è¯Šæ–­ç»“æœï¼ˆscoreã€error_typesã€action_vectorã€candidate_injectsï¼‰
ç”Ÿæˆæ³¨å…¥å†…å®¹å¹¶æ§åˆ¶æ”¶æ•›ï¼ŒæŸå¤±ä¸ºè¯­ä¹‰æŸå¤± loss = 1 - scoreã€‚

å®‰å…¨çº¦æŸï¼šä¸æ³„éœ²ç­”æ¡ˆåˆ°æ³¨å…¥å†…å®¹ä¸­ã€‚
"""

import re
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from enum import Enum
import random

try:
    from .semantic_gradient import SemanticGradient
except ImportError:
    from semantic_gradient import SemanticGradient


class ErrorType(Enum):
    """é”™è¯¯ç±»å‹åˆ†ç±»"""

    FIELD_ERROR = "field_error"  # å­—æ®µä½¿ç”¨é”™è¯¯
    CALCULATION_ERROR = "calc_error"  # è®¡ç®—é€»è¾‘é”™è¯¯
    FORMAT_ERROR = "format_error"  # è¾“å‡ºæ ¼å¼é”™è¯¯
    INCOMPLETE = "incomplete"  # ç»“æœä¸å®Œæ•´
    MAGNITUDE_ERROR = "magnitude"  # æ•°é‡çº§é”™è¯¯
    LOGIC_ERROR = "logic_error"  # é€»è¾‘æ¨ç†é”™è¯¯
    TIMEOUT_ERROR = "timeout_error"  # æ‰§è¡Œè¶…æ—¶
    UNKNOWN = "unknown"


@dataclass
class FailureRecord:
    """å¤±è´¥è®°å½•"""

    iteration: int
    inject_content: str
    actual_output: str
    error_type: ErrorType
    error_features: Dict
    loss: float


@dataclass
class OptimizationInfo:
    """ä¼˜åŒ–ä¿¡æ¯"""

    gradient: Dict
    learning_rate: float
    convergence_status: str
    loss: float
    momentum_strength: float


class InjectsOptimizer:
    """
    åŸºäºæ¢¯åº¦ä¸‹é™æ€æƒ³çš„æ³¨å…¥å†…å®¹ä¼˜åŒ–å™¨
    """

    def __init__(
        self,
        learning_rate: float = 1.0,
        momentum: float = 0.9,
        patience: int = 3,
        min_learning_rate: float = 0.1,
    ):
        self.learning_rate = learning_rate
        self.initial_learning_rate = learning_rate
        self.momentum = momentum
        self.patience = patience
        self.min_learning_rate = min_learning_rate

        # çŠ¶æ€è®°å½•
        self.velocity = {}
        self.failure_history: List[FailureRecord] = []
        self.loss_history: List[float] = []
        self.best_loss = float("inf")
        self.plateau_count = 0

        # Baselineè®°å½•
        self.baseline_result = None
        self.baseline_loss = None

        # è¯­ä¹‰é©±åŠ¨ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
        self._semantic_judge: Any | None = None
        self._semantic_analysis_content: str = ""
        self._semantic_knowledge: str = ""

    def optimize(
        self,
        actual: str,
        expected: str,
        knowledge_base: str,
        iteration: int,
        previous_inject: str = "",
    ) -> Tuple[str, OptimizationInfo]:
        """
        ä¸»ä¼˜åŒ–å‡½æ•°
        è¿”å›: (ä¼˜åŒ–åçš„æ³¨å…¥å†…å®¹, ä¼˜åŒ–ä¿¡æ¯)
        """
        # è¯­ä¹‰é©±åŠ¨ï¼ˆå¿…é¡»å¯ç”¨ï¼‰
        if not (self._semantic_judge is not None):
            raise RuntimeError(
                "Semantic mode not enabled. Call enable_semantic(...) before optimize()."
            )

        judge_grad_raw = self._semantic_judge.evaluate(
            analysis_content=self._semantic_analysis_content or "",
            expected=expected or "",
            actual=actual or "",
            knowledge=self._semantic_knowledge or knowledge_base or "",
        )

        semantic_grad = SemanticGradient.from_judge_result(judge_grad_raw)
        gradient = self._build_semantic_gradient(semantic_grad, actual, expected)
        loss = semantic_grad.loss
        self.loss_history.append(loss)
        self._update_momentum(gradient)
        adaptive_lr = self._get_adaptive_learning_rate(iteration)
        new_inject = self._generate_inject_from_gradient(
            gradient, knowledge_base, iteration, adaptive_lr
        )
        self._audit_inject(new_inject, expected)
        self._record_failure(iteration, new_inject, actual, gradient, loss)
        opt_info = OptimizationInfo(
            gradient=gradient,
            learning_rate=adaptive_lr,
            convergence_status=self._check_convergence(),
            loss=loss,
            momentum_strength=self._get_momentum_strength(),
        )
        return new_inject, opt_info

    def enable_semantic(self, judge: Any, analysis_content: str, knowledge: str = ""):
        """å¯ç”¨è¯­ä¹‰é©±åŠ¨æ¨¡å¼ã€‚

        Args:
            judge: SemanticJudge å®ä¾‹ï¼ˆéœ€æä¾› evaluate æ¥å£ï¼‰
            analysis_content: è·¨ run æ±‡æ€»åˆ†æ
            knowledge: ä¸šåŠ¡çŸ¥è¯†æ–‡æœ¬
        """
        self._semantic_judge = judge
        self._semantic_analysis_content = analysis_content or ""
        self._semantic_knowledge = knowledge or ""

    def _build_semantic_gradient(
        self, semantic_grad: SemanticGradient, actual: str, expected: str
    ) -> Dict:
        """å°† SemanticGradient æ˜ å°„ä¸ºå†…éƒ¨æ¢¯åº¦ç»“æ„ã€‚"""
        score = semantic_grad.score
        error_types = semantic_grad.error_types
        action_vector = semantic_grad.action_vector
        candidate_injects = semantic_grad.candidate_injects

        et = self._map_semantic_error_to_enum(
            error_types[0] if error_types else "unknown"
        )
        gradient = {
            "has_output": bool(actual and actual.strip()),
            "is_error": self._is_error_output(actual),
            "looks_complete": not (actual or "").endswith(("...", "çœç•¥")),
            "has_content": len(actual or "") > 20,
            "semantic_hint": (
                ("needs_refinement" if score < 0.6 else "ok") if actual else "no_output"
            ),
            "error_type": et,
            "magnitude": 1.0 - score,
            "action_vector": action_vector,
            "candidate_injects": candidate_injects,
            "score": score,
            "raw": semantic_grad.to_dict(),
        }

        if self.baseline_result is not None and self.baseline_loss is not None:
            current_loss = 1.0 - score
            baseline_loss = self.baseline_loss
            gradient["improved_from_baseline"] = current_loss < baseline_loss
            gradient["improvement_ratio"] = (
                (baseline_loss - current_loss) / baseline_loss
                if baseline_loss > 0
                else 0
            )
            gradient["baseline_comparison"] = {
                "improvement": gradient["improvement_ratio"],
                "baseline_loss": baseline_loss,
                "current_loss": current_loss,
                "is_better": current_loss < baseline_loss,
                "degradation_ratio": (
                    (current_loss - baseline_loss) / baseline_loss
                    if baseline_loss > 0
                    else 0
                ),
            }

        return gradient

    def _map_semantic_error_to_enum(self, err: str) -> ErrorType:
        """å°†è¯­ä¹‰è£åˆ¤çš„é”™è¯¯ç±»å‹å­—ç¬¦ä¸²æ˜ å°„ä¸ºå†…éƒ¨æšä¸¾ã€‚"""
        if not err:
            return ErrorType.UNKNOWN
        e = err.lower()
        if any(k in e for k in ["calc", "è®¡ç®—", "æ•°å€¼", "å…¬å¼"]):
            return ErrorType.CALCULATION_ERROR
        if any(k in e for k in ["å­—æ®µ", "field", "åˆ—", "ç»´åº¦"]):
            return ErrorType.FIELD_ERROR
        if any(k in e for k in ["æ ¼å¼", "format", "è¾“å‡ºæ ¼å¼"]):
            return ErrorType.FORMAT_ERROR
        if any(k in e for k in ["ä¸å®Œæ•´", "ç¼ºå¤±", "incomplete"]):
            return ErrorType.INCOMPLETE
        if any(k in e for k in ["æ•°é‡çº§", "magnitude"]):
            return ErrorType.MAGNITUDE_ERROR
        if any(k in e for k in ["é€»è¾‘", "reasoning", "logic"]):
            return ErrorType.LOGIC_ERROR
        if any(k in e for k in ["è¶…æ—¶", "timeout"]):
            return ErrorType.TIMEOUT_ERROR
        return ErrorType.UNKNOWN

    def set_baseline(self, baseline_result: str, baseline_loss: float):
        """è®¾ç½®baselineä»¥ä¾›å¯¹æ¯”"""
        self.baseline_result = baseline_result
        self.baseline_loss = baseline_loss
        print(f"ğŸ“Š è®¾ç½®baseline: æŸå¤±={baseline_loss:.4f}")

    # å·²ç§»é™¤ï¼šåŸºäºè¡¨å±‚ç»Ÿè®¡çš„æŸå¤±å‡½æ•°ï¼Œä½¿ç”¨è¯­ä¹‰æŸå¤±ï¼ˆ1 - scoreï¼‰

    # å·²ç§»é™¤ï¼šå¯å‘å¼æ¢¯åº¦è®¡ç®—ï¼ˆç»Ÿä¸€æ”¹ä¸ºè¯­ä¹‰è£åˆ¤é©±åŠ¨ï¼‰

    def _is_error_output(self, output: str) -> bool:
        """ç®€å•åˆ¤æ–­æ˜¯å¦æ˜¯é”™è¯¯è¾“å‡º"""
        if not output:
            return True
        lower_output = output.lower()
        return any(
            err in lower_output
            for err in ["error", "exception", "failed", "é”™è¯¯", "å¤±è´¥"]
        )

    def _count_stuck_iterations(self) -> int:
        """è®¡ç®—å¡ä½çš„è¿­ä»£æ¬¡æ•°"""
        if len(self.failure_history) < 2:
            return 0

        # è®¡ç®—è¿ç»­ç›¸åŒé”™è¯¯çš„æ¬¡æ•°
        current_error = self.failure_history[-1].error_type
        stuck_count = 0
        for record in reversed(self.failure_history):
            if record.error_type == current_error:
                stuck_count += 1
            else:
                break
        return stuck_count

    # å·²ç§»é™¤ï¼š_classify_error/_classify_error_simple ä¸åŸºç¡€ç‰¹å¾æ–¹æ³•ï¼ˆè¯­ä¹‰é©±åŠ¨ä¸‹ä¸éœ€è¦ï¼‰

    # å·²ç§»é™¤ï¼šå¯å‘å¼ä¼˜åŒ–æ–¹å‘ï¼ˆç”±è¯­ä¹‰è£åˆ¤çš„ action_vector/candidate_injects å–ä»£ï¼‰

    def _update_momentum(self, gradient: Dict):
        """æ›´æ–°åŠ¨é‡"""
        error_type_key = gradient["error_type"].value

        if "error_type" not in self.velocity:
            self.velocity["error_type"] = {}

        if error_type_key not in self.velocity["error_type"]:
            self.velocity["error_type"][error_type_key] = 0

        # æ›´æ–°åŠ¨é‡
        self.velocity["error_type"][error_type_key] = (
            self.momentum * self.velocity["error_type"][error_type_key]
            + (1 - self.momentum) * gradient["magnitude"]
        )

    def _generate_inject_from_gradient(
        self, gradient: Dict, knowledge_base: str, iteration: int, learning_rate: float
    ) -> str:
        """
        æ³¨å…¥ç”Ÿæˆï¼šä¼˜å…ˆé‡‡ç”¨è¯­ä¹‰å€™é€‰ä¸è¡ŒåŠ¨å‘é‡ï¼Œå…¶æ¬¡å›é€€åˆ° hint ç»„åˆ
        """
        # ä¼˜å…ˆç›´æ¥é‡‡ç”¨å€™é€‰æ³¨å…¥
        cand = gradient.get("candidate_injects") or []
        if isinstance(cand, list) and cand:
            return cand[0]

        # å…¶æ¬¡é‡‡ç”¨è¡ŒåŠ¨å‘é‡
        actions = gradient.get("action_vector") or []
        if isinstance(actions, list) and actions:
            return "ï¼›".join(actions)

        inject_parts = []

        # åŸºäºè¯­ä¹‰æç¤ºçš„ç®€å•æŒ‡å¯¼
        semantic_hint = gradient.get("semantic_hint", "needs_refinement")

        if semantic_hint == "no_output":
            inject_parts.append("è¯·ç¡®ä¿æä¾›æœ‰æ•ˆçš„è¾“å‡ºç»“æœ")
        elif semantic_hint == "execution_error":
            inject_parts.append("è¯·æ£€æŸ¥å¹¶ä¿®æ­£æ‰§è¡Œè¿‡ç¨‹ä¸­çš„é”™è¯¯")
        elif semantic_hint == "incomplete_response":
            inject_parts.append("è¯·ç¡®ä¿è¾“å‡ºå®Œæ•´çš„åˆ†æç»“æœ")
        elif semantic_hint == "too_brief":
            inject_parts.append("è¯·æä¾›æ›´è¯¦ç»†çš„åˆ†æå’Œè¯´æ˜")
        else:
            inject_parts.append("è¯·ä»”ç»†æ£€æŸ¥åˆ†æè´¨é‡ï¼Œç¡®ä¿å‡†ç¡®æ€§")

        # åŸºäºå¤±è´¥æ¬¡æ•°è°ƒæ•´ç­–ç•¥
        failure_count = gradient.get("failure_count", 0)
        if failure_count > 2:
            inject_parts.append("è¯·å°è¯•ä¸åŒçš„åˆ†ææ–¹æ³•")

        stuck_count = gradient.get("stuck_iterations", 0)
        if stuck_count > 1:
            inject_parts.append("è¯·ä»æ–°çš„è§’åº¦é‡æ–°æ€è€ƒé—®é¢˜")

        # æ·»åŠ çŸ¥è¯†åº“å†…å®¹ï¼ˆç®€åŒ–å¤„ç†ï¼‰
        if knowledge_base and len(knowledge_base) > 50:
            # ç›´æ¥ä½¿ç”¨çŸ¥è¯†åº“çš„å‰200å­—ç¬¦ï¼Œè®©LLMè‡ªå·±ç†è§£ç›¸å…³æ€§
            knowledge_excerpt = knowledge_base[:200].strip()
            if knowledge_excerpt:
                inject_parts.append(f"å‚è€ƒè¦ç‚¹ï¼š{knowledge_excerpt}")

        # åŸºäºbaselineæ”¹è¿›æƒ…å†µçš„æŒ‡å¯¼
        if gradient.get("improved_from_baseline", False):
            improvement = gradient.get("improvement_ratio", 0)
            if improvement > 0.3:
                inject_parts.append("å½“å‰æ–¹å‘æ­£ç¡®ï¼Œç»§ç»­ä¼˜åŒ–")
            else:
                inject_parts.append("æœ‰æ‰€æ”¹è¿›ä½†ä»éœ€è¿›ä¸€æ­¥ä¼˜åŒ–")
        elif gradient.get("improvement_ratio", 0) < 0:
            inject_parts.append("å½“å‰æ–¹æ³•å¯èƒ½æœ‰é—®é¢˜ï¼Œè¯·è°ƒæ•´ç­–ç•¥")

        # ç»„åˆæ‰€æœ‰éƒ¨åˆ†
        base_inject = (
            "ï¼›".join(inject_parts) if inject_parts else "è¯·ä»”ç»†åˆ†æå¹¶æä¾›å‡†ç¡®ç»“æœ"
        )

        # æ ¹æ®è¿­ä»£æ¬¡æ•°å¢å¼ºå¼ºè°ƒ
        if iteration > 0:
            base_inject = f"ç¬¬{iteration + 1}æ¬¡æé†’ï¼š{base_inject}"

        return base_inject

    # å·²ç§»é™¤ï¼šçŸ¥è¯†æå–/å†å²å­¦ä¹ ç­‰å¯å‘å¼è¾…åŠ©æ–¹æ³•

    def _generate_baseline_guidance(self, baseline_comparison: Dict) -> str:
        """åŸºäºbaselineå¯¹æ¯”ç”ŸæˆæŒ‡å¯¼"""
        if not baseline_comparison:
            return ""

        guidance_parts = []

        # æ ¹æ®æ”¹è¿›/é€€åŒ–å¹…åº¦æä¾›é€šç”¨æŒ‡å¯¼
        if not baseline_comparison.get("is_better", False):
            degradation_ratio = baseline_comparison.get("degradation_ratio", 0)
            if degradation_ratio > 0.2:
                guidance_parts.append("å½“å‰æ–¹æ³•å¯èƒ½å¯¼è‡´ç»“æœé€€åŒ–ï¼Œè¯·è°ƒæ•´ç­–ç•¥")
            else:
                guidance_parts.append("æ”¹è¿›æœ‰é™ï¼Œå»ºè®®å°è¯•ä¸åŒçš„æ¨ç†è·¯å¾„")
        else:
            guidance_parts.append("æ–¹å‘æ­£ç¡®ï¼Œå»ºè®®ç»§ç»­æ²¿æ­¤æ–¹å‘ç»†åŒ–")

        # å¦‚æœæ”¹è¿›å¾ˆå°ï¼Œæä¾›æ›´æ¿€è¿›çš„ç­–ç•¥
        improvement = baseline_comparison.get("improvement", 0)
        if 0 < improvement < 0.1:  # æ”¹è¿›å¾ˆå°
            guidance_parts.append("éœ€è¦æ›´å¤§çš„ç­–ç•¥è°ƒæ•´æ¥å®ç°çªç ´")

        if guidance_parts:
            return f"åŸºäºbaselineå¯¹æ¯”ï¼š{' '.join(guidance_parts)}"
        return ""

    # å·²ç§»é™¤ï¼šå¤±è´¥æ¨¡å¼ä¸é‡å¤é”™è¯¯å¯å‘å¼ï¼ˆè¯­ä¹‰é©±åŠ¨ä¸‹ä¸éœ€è¦ï¼‰

    def _get_adaptive_learning_rate(self, iteration: int) -> float:
        """è‡ªé€‚åº”å­¦ä¹ ç‡"""
        # åŸºç¡€è¡°å‡
        decay_rate = 0.9
        base_lr = self.initial_learning_rate * (decay_rate**iteration)

        # æ ¹æ®æ”¶æ•›çŠ¶æ€è°ƒæ•´
        if self._check_convergence() == "stuck":
            base_lr *= 1.5  # å¢åŠ æ¢ç´¢
        elif self._check_convergence() == "oscillating":
            base_lr *= 0.5  # å‡å°‘éœ‡è¡

        return max(base_lr, self.min_learning_rate)

    def _check_convergence(self) -> str:
        """æ£€æŸ¥æ”¶æ•›çŠ¶æ€"""
        if len(self.loss_history) < 3:
            return "initializing"

        recent_losses = self.loss_history[-3:]

        # æ£€æŸ¥æ˜¯å¦å¡ä½
        if len(self.failure_history) >= 2:
            recent_errors = [f.error_type for f in self.failure_history[-2:]]
            if recent_errors[0] == recent_errors[1]:
                return "stuck"

        # æ£€æŸ¥æ˜¯å¦éœ‡è¡
        if len(recent_losses) >= 3:
            if (
                recent_losses[0] < recent_losses[1] > recent_losses[2]
                or recent_losses[0] > recent_losses[1] < recent_losses[2]
            ):
                return "oscillating"

        # æ£€æŸ¥æ˜¯å¦æ”¹å–„
        if recent_losses[-1] < recent_losses[0]:
            return "improving"

        return "plateau"

    def should_early_stop(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ"""
        if len(self.loss_history) < self.patience:
            return False

        # æ£€æŸ¥æœ€è¿‘å‡ æ¬¡æ˜¯å¦æ²¡æœ‰æ”¹å–„
        recent_losses = self.loss_history[-self.patience :]
        if max(recent_losses) - min(recent_losses) < 0.01:
            self.plateau_count += 1
            return self.plateau_count >= 2

        self.plateau_count = 0
        return False

    def random_exploration(self, knowledge_base: str) -> str:
        """éšæœºæ¢ç´¢æ–°ç­–ç•¥"""
        strategies = [
            "è¯·ä»ä¸åŒè§’åº¦é‡æ–°åˆ†æé—®é¢˜",
            "å»ºè®®ç®€åŒ–æŸ¥è¯¢é€»è¾‘ï¼Œåˆ†æ­¥éª¤æ‰§è¡Œ",
            "è¯·æ£€æŸ¥æ˜¯å¦é—æ¼äº†å…³é”®çš„è¿‡æ»¤æ¡ä»¶",
            "å°è¯•ä½¿ç”¨ä¸åŒçš„æ•°æ®èšåˆæ–¹å¼",
        ]

        base_strategy = random.choice(strategies)

        if knowledge_base and len(knowledge_base) > 100:
            # éšæœºé€‰æ‹©çŸ¥è¯†åº“çš„ä¸€éƒ¨åˆ†
            start_pos = random.randint(0, len(knowledge_base) - 100)
            knowledge_fragment = knowledge_base[start_pos : start_pos + 200]
            return f"{base_strategy}ã€‚å‚è€ƒçŸ¥è¯†ï¼š{knowledge_fragment}"

        return base_strategy

    def _record_failure(
        self,
        iteration: int,
        inject_content: str,
        actual_output: str,
        gradient: Dict,
        loss: float,
    ):
        """è®°å½•å¤±è´¥ä¿¡æ¯"""
        record = FailureRecord(
            iteration=iteration,
            inject_content=inject_content,
            actual_output=actual_output[:500],  # é™åˆ¶é•¿åº¦
            error_type=gradient["error_type"],
            error_features=gradient.get("features", {}),
            loss=loss,
        )
        self.failure_history.append(record)

        # åªä¿ç•™æœ€è¿‘çš„è®°å½•
        if len(self.failure_history) > 10:
            self.failure_history = self.failure_history[-10:]

    def _audit_inject(self, inject_content: str, expected: str):
        """å®¡è®¡æ³¨å…¥å†…å®¹ï¼Œç¡®ä¿ä¸åŒ…å«ç­”æ¡ˆ"""
        # æ£€æŸ¥ç›´æ¥çš„ç­”æ¡ˆå…³é”®è¯
        dangerous_patterns = [
            r"ç­”æ¡ˆæ˜¯",
            r"ç»“æœæ˜¯.*\d",  # åªåŒ¹é…"ç»“æœæ˜¯"åé¢è·Ÿæ•°å­—çš„æƒ…å†µ
            r"åº”è¯¥æ˜¯",
            r"æ­£ç¡®ç­”æ¡ˆ",
            r"è®¡ç®—ç»“æœ.*\d",  # åªåŒ¹é…"è®¡ç®—ç»“æœ"åé¢è·Ÿæ•°å­—çš„æƒ…å†µ
        ]

        for pattern in dangerous_patterns:
            if re.search(pattern, inject_content):
                raise ValueError(f"æ³¨å…¥å†…å®¹åŒ…å«å±é™©æ¨¡å¼: {pattern}")

        # æå–ç­”æ¡ˆä¸­çš„å…³é”®ç‰‡æ®µï¼ˆé¿å…çŸ­è¯è¯¯åˆ¤ï¼‰
        answer_fragments = []
        words = expected.split()
        for i in range(len(words)):
            for j in range(i + 2, min(i + 6, len(words) + 1)):  # 2-5ä¸ªè¯çš„ç‰‡æ®µ
                fragment = " ".join(words[i:j])
                if len(fragment) > 8:  # åªæ£€æŸ¥è¾ƒé•¿çš„ç‰‡æ®µï¼Œæé«˜é˜ˆå€¼
                    answer_fragments.append(fragment)

        for fragment in answer_fragments:
            if fragment.lower() in inject_content.lower():
                raise ValueError(f"æ³¨å…¥å†…å®¹å¯èƒ½æ³„éœ²ç­”æ¡ˆç‰‡æ®µ: {fragment[:20]}...")

        # æ£€æŸ¥å…·ä½“æ•°å€¼ï¼ˆ3ä½ä»¥ä¸Šï¼Œé™ä½é˜ˆå€¼æé«˜å®‰å…¨æ€§ï¼‰
        expected_numbers = re.findall(r"\d{3,}", expected)
        inject_numbers = re.findall(r"\d{3,}", inject_content)

        for num in inject_numbers:
            if num in expected_numbers:
                raise ValueError(f"æ³¨å…¥å†…å®¹åŒ…å«ç­”æ¡ˆä¸­çš„å…·ä½“æ•°å€¼: {num}")

        # æ£€æŸ¥ç™¾åˆ†æ¯”
        expected_percentages = re.findall(r"\d+%", expected)
        inject_percentages = re.findall(r"\d+%", inject_content)

        for pct in inject_percentages:
            if pct in expected_percentages:
                raise ValueError(f"æ³¨å…¥å†…å®¹åŒ…å«ç­”æ¡ˆä¸­çš„å…·ä½“ç™¾åˆ†æ¯”: {pct}")

        return True

    # å·²ç§»é™¤ï¼šæ ¼å¼ç±»å‹æ£€æµ‹ï¼ˆå¯å‘å¼ï¼‰

    # å·²ç§»é™¤ï¼šæ ¼å¼/ç»“æ„ç›¸å…³çš„å¯å‘å¼å‡½æ•°

    # å·²ç§»é™¤ï¼šç»“æ„/å®Œæ•´æ€§/æ•°å€¼å­˜åœ¨æ€§/è¯¯å·®é‡çº§ç­‰å¯å‘å¼å‡½æ•°

    def _analyze_previous_inject(self, previous_inject: str) -> Dict:
        """åˆ†æä¹‹å‰çš„æ³¨å…¥å†…å®¹"""
        if not previous_inject:
            return {}

        return {
            "length": len(previous_inject),
            "has_emphasis": any(
                word in previous_inject for word in ["é‡è¦", "æ³¨æ„", "å…³é”®"]
            ),
            "has_specific_guidance": len(
                re.findall(r"å­—æ®µ|è¡¨|è®¡ç®—|å…¬å¼", previous_inject)
            )
            > 0,
            "iteration_mentioned": bool(re.search(r"ç¬¬\d+æ¬¡", previous_inject)),
        }

    def _combine_inject_components(self, components: List[str]) -> str:
        """ç»„åˆæ³¨å…¥ç»„ä»¶"""
        if not components:
            return "è¯·ä»”ç»†åˆ†æé—®é¢˜å¹¶ç»™å‡ºå‡†ç¡®ç»“æœ"

        # å»é‡å¹¶ç»„åˆ
        unique_components = list(dict.fromkeys(components))  # ä¿æŒé¡ºåºçš„å»é‡
        return "ï¼›".join(unique_components)

    # å·²ç§»é™¤ï¼šå¯å‘å¼ baseline å¯¹æ¯”ï¼ˆè¯­ä¹‰æ¨¡å¼ç”± _build_semantic_gradient å†…éƒ¨å®Œæˆï¼‰

    def _get_momentum_strength(self) -> float:
        """è·å–å½“å‰åŠ¨é‡å¼ºåº¦"""
        if not self.velocity.get("error_type"):
            return 0.0

        return sum(self.velocity["error_type"].values()) / len(
            self.velocity["error_type"]
        )

    def get_optimization_summary(self) -> Dict:
        """è·å–ä¼˜åŒ–æ€»ç»“"""
        if not self.failure_history:
            return {}

        error_types = [f.error_type.value for f in self.failure_history]
        error_counts = {et: error_types.count(et) for et in set(error_types)}

        return {
            "total_iterations": len(self.failure_history),
            "error_distribution": error_counts,
            "loss_trend": (
                self.loss_history[-5:]
                if len(self.loss_history) >= 5
                else self.loss_history
            ),
            "best_loss": self.best_loss,
            "final_convergence_status": self._check_convergence(),
            "momentum_info": self.velocity,
        }
